---
title: "Inferring cultural transmission from frequency data using the *cTransmission* package"
author: "Enrico Crema, Anne Kandler"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    toc: true
    fig_caption: true
    self_contained: yes
fontsize: 11pt
documentclass: article
vignette: >
  %\VignetteIndexEntry{Analysing Radiocarbon dates with the *rcarbon* package}
  %\VignetteEngine{knitr::rmarkdown_notangle}
---


```{r, include = FALSE}
h = 3.5
w = 3.5
is_check <- ("CheckExEnv" %in% search()) || any(c("_R_CHECK_TIMINGS_",
             "_R_CHECK_LICENSE_") %in% names(Sys.getenv()))
knitr::opts_chunk$set(fig.align = "center", eval = !is_check)
library(rcarbon)
```

```{r, include = FALSE}
devtools::load_all()
```

# Introduction

The *cTransmission* package provides a framework for creating simulation models that can be used to infer &mdash via approximate bayesian computation (ABC) &mdash patterns of social learning from observed cultural frequency data. The actual inferential process is can be carried out in a variety of ways, this vignette will illustrate a workflow based on the *EasyABC* package, which provides multiple flavours of ABC.

We'll start by installing *cTransmission* and the *EasyABC* package:

```{r,eval=FALSE}
devtools::install_github('ercrema/cTransmission')
install.packages('EasyABC')
```

and load them

```{r,results='hide'}
library(cTransmission)
library(EasyABC)
```

# Data Preparation

Simulation models generated in *cTransmission* are tailored to specific, user-provided, datasets that are used to inform several model parameters, the initial condition of the system, and the "target" used for the ABC inference. 

In order to generate a suitable dataset for *cTransmission* we need: 1) A matrix containing the raw frequencies of each variant across multiple temporal sampling windows (sampling phases); 2) the times-stamp of the last time-step of each sampling window; and 3) the durration of the sampling windows for all phases except the first. We'll start by creating a matrix of variants frequencies:

<!-- The script below generates cultural frequency data with a weak conformist bias -->
```{r,eval=FALSE,echo=FALSE}
# Parameters ####
set.seed(1)
b=0.01
r=0.2
s=0.2
mu=0.005


#Setup ####
n.phases=3
durations=c(1,10,10)
timestamps=c(1,30,60)
iniPop = c(10, 20, 50, 100, 120, 250, 450, 500) #initial population frequencies
popAtSamplePhases = c(sum(iniPop),1000,1200) #population sizes at different sampling phases
popSizes = round(approx(x=c(1,30,60),y=popAtSamplePhases,xout=1:60)$y) #population size at each timestep
timesteps = max(timestamps) #number of timesteps

output = vector("list",length=timesteps) #define output list
counter = length(iniPop)+1 #set up counter for innovation
output[[1]]= rep(1:length(iniPop),times=iniPop) #intial population (raw)

# Simulation ####
for (i in 2:timesteps)
{
  # compute changes in population
  if (popSizes[i]<popSizes[i-1])
  {
    toAdd = floor(popSizes[i]*r)
    toRemove = toAdd + popSizes[i-1] - popSizes[i]
  }
  if (popSizes[i]>popSizes[i-1])
  {
    toRemove = floor(popSizes[i-1]*r)
    toAdd = toRemove + popSizes[i] - popSizes[i-1]
  }
  # random removal of individuals
  output[[i]] = output[[i-1]][-sample(1:popSizes[i-1],size=toRemove)]
  # random addition of individuals 
  p=table(output[[i-1]])/pop
  t=as.numeric(names(p))
  toBeAdded =  sample(t,toAdd,replace=TRUE,prob=p^(1+b)/sum(p^(1+b)))
  index = which(runif(toAdd)<mu)
  if (length(index)>0)
  {
    toBeAdded[index]=counter:c(counter+length(index)-1)
    counter = max(toBeAdded)+1
  }
  output[[i]] = c(output[[i]],toBeAdded)
}


# Sampling ####
sample = vector("list",length=n.phases)
sample[[1]] = sample(output[[1]],size=popAtSamplePhases[1]*s)

for (i in 2:c(n.phases))
{
  sample[[i]] =  sample(unlist(output[c(timestamps[i]-durations[i]+1):timestamps[i]]),size=popAtSamplePhases[i]*s)
}

(obs=t(sapply(sample,instances,variants=var)))
```

```{r}
# Create an artificial dataset
obs = rbind(c(2, 4, 7, 16, 27, 44, 96, 104, 0, 0, 0, 0),
            c(3, 2, 2, 19, 10, 27, 73,  62, 1, 1, 0 ,0),
            c(0, 4, 0, 17, 7,  47, 80,  82, 0, 0, 2, 1))
colnames(obs) = LETTERS[1:ncol(obs)]
```

The matrix `obs` contains the raw frequencies of `r ncol(obs)` cultural variants collected across `r nrow(obs)` sampling phases. The values of the matrix have been generated running a simulation within R, readers interested in the details of the simulation can access the source markdown file of this document. Once we have our matrix with variants frequencies we can run the `cFreqData()` function which will create an object of class `cFreqData` data which will be required for other functions of the package:


```{r}
x = cFreqData(x = obs,timestamp = c(1,30,60), duration = c(10,10))
```

The argument `timestamp` requires a vector of numbers indicating the last timestep of each sampling phase, whilst the argument `duration` contains the duration of each sampling phase. This effectively implies that we are assuming that our variant frequency was generated by 60 'generations' of transmission processes, and that our second phase is a sample of variants produced in generations 21 to 30 and our third phase a sample of variants produced in generations 51 to 60. Of course translating calendar time (e.g. 100 years) into timesteps is not a trivial exercise, and requires additional assumptions on the number of transmission of events over time.


# Creating the simulation model

Simulation models produced in `cTransmission` can be summarised in the follow steps:

1. Infer population size across all time-steps assuming that observed variants are just a fraction $s$ of the opulation.
2. Define the proportion $r$ of the population engaged into cultural transmission.
3. Infer the population frequencies of the cultural variants in the first phase
4. Utilise a user define transmission model and associated parameters to infer the expected frequencies of cultural variants for all sampling phases except the first, taking into account sampling error and time-averaging.

The `genSim()` function provides a wrapper for all these steps and generates a bespoke R function which can be used for ABC. The example below creates the function `sim()`:


```{r}
sim = genSim(theta=c("mu","b"),x=x,model=frequencyBias,sMean=0.2,sVariance=0.05,alpha=1,rMean=0.2,rVariance=0.05)
```

The function requires the following arguments:

* `x` The observed frequency data, timestamps, and durations (i.e. a `CFreqData` class object)
* `sMean` the sampling fraction
* `sVariance` the uncertainty in the estimated sampling fraction. If `sVariance` is larger than 0, a random value will be sampled from a truncated normal distribution with mean `sMean`, standard deviation `sVariance` and values between 0 and 1.
* `rMean` the replacement rate
* `rVariance` the uncertainty of the estimated replacement rate. If `rVariance` is larger than 0, a random value will be sampled from a truncated normal distribution with mean `rMean`, standard deviation `rVariance` and values between 0 and 1.
* `alpha` the prior for the Dirichlet 
* `model` a transmission model
* `theta` a vector of the names of the parameters to be estimated. 

The resulting function reads a list of parameters values (those defined by the argument `theta`) and produces the expected frequency of cultural variants that are observed during the first phase across the remaining phases. For example:

```{r}
(res=sim(list(0.05,-0.01)))
```

will run a `frequencyBias` transmission model with a mutation rate of 0.05 and a frequency bias of -0.01 (a mild anti-conformist bias) and generate the frequencies of the `r x$k[1]` cultural variants observed in our first phase. This can then be compared to the observed and simulated frequencies:

```{r}
#observed
round(prop.table(x$cfreq,1),3)[2:3,1:x$k[1]]
#simulated
round(matrix(res,ncol=8,nrow=2),3)
```

Notice that the argument `theta` can also contain the sampling fraction and/or the replacement rate. For example:

```{r}
sim2 = genSim(theta=c("s","mu","b"),x=x,model=frequencyBias,alpha=1,rMean=0.2,rVariance=0.05)
```

will generate a function (`sim2()`) which would require as argument the sampling fraction `s`, the mutation rate `mu`, and the frequency bias parameter `b`. Notice that the order with which these parameter are supplied is important, as the output functions of `genSim()` cannot accept named argument.

The next few subsection provides some further details on the internal functions of `genSim()` and [how to generate bespoke transmission models][Creating a bespoke transmission model]. Users interested in making ABC inference with the default frequency bias transmission model can skip to the section [Approximate Bayesian Computation via the *EasyABC* package].  

## Estimating Population Sizes

Population size estimate is handled internally by the function `generate_popSize()` which requires as argument the observed frequencies (supplied as a `cFreqData` class object) and the parameters `sMean` and `sVariance`. The function first estimates the sampling fraction $s$ as a random draw from a truncated normal distribution with mean `sMean` and standard deviation `sVariance`. Then it estimates the population size at each sampling phase by dividing the observed sample size by $s$, and computes a linear interpolation accross all time steps. For example if we assume $s=0.2$ we would have: 

```{r,fig.width=5,fig.height=5}
(pop=generate_popSize(x = x,sMean = 0.2,sVariance = 0))
plot(pop,type='b',xlab="timesteps",ylab="estimated N")
points(x$tstamp,x$n/0.2,pch=20,cex=1.2) #x$n is the sample size for each sampling phase, x$tstamp is the timestamp of each sampling phase
legend("topright",legend=c("Timestamp at sampling phases","Interpolated timestamps"),pch=c(20,1),pt.cex=c(1.2,1))
```

where `pop` is vector with length `r length(pop)`, corresponding to the number of timesteps we assumed. 

## Computing the number of removal/additions

The `cTransmission` package accounts for the possibility that only a proportion $r$ of the population engage with cultural transmission. The `generate_removalReplacement()` function computes the number of new observations introduced (via social learning) and removed (by random selection) at each timestep. The function requires the `cFreqData` class object, the estimated population size, and the replacement rate $r$:

```{r}
rr=generate_removalReplacement(x=x,N=pop,r = 0.2)
head(rr)
```

The resulting data.frame contains the number of objects to be removed (`u`) or and added (`v`) at each timestep.

<!-- NOTE for Anne: Should we allow user defined vector of population values so we can consider situations like Merzbach where we have independent estimates? -->

## Estimating Variants Frequencies of the first phase

The observed frequency of cultural variants play a pivotal role in the inferential process offered by `cTransmission` as it defines the initial conditions of the simulation model created by the `genSim()` function. However the obeserved frequencies decribe the sample and not the population of cultural variants. The `generate_initialPop()` function utilises a Dirichlet distribution approach to estimate the frequency of each cultural variant at the population level. The function requires a vector of observed frequencies and the parameter $\alpha$ of the Dirichlet distribution. Internally `genSim()` supplies a vector of $k+1$ variants, where $k$ is the number of cultural variants observed during the first phase. The additional $+1$ is a placeholder for unobserved variants.

```{r}
observedFreq=x$cfreq[1,1:x$k[1]] #extract raw frequencies of observed variant
observedFreq=c(observedFreq,0) #add placeholder for unobserved variants
popFreq=generate_initialPop(observedFreq,alpha=1) # use the Dirichlet distribution to estimate one possible set of variant frequencies
ini = round(popFreq*pop[1]) #compute initial raw frequencies by combining estimates of population size and estimates of variant relative frequencies obtained from generate_initialPop
```

## Modelling cultural change

Once estimates of population structure and size are estimated, expected changes in the frequency of cultural variants are computed by the `generate_cultural_change()` function. This is actually a wrapper function that combines and feeds the output of `generate_popSize()`, `generate_removalReplacement()`, and `generate_initialPop()` to a transmission model defined by the arguments `model` and `params`. The former is a function (in this case `frequencyBias()`) that reads variant frequencies and computes the probability of each variant being selected under a particular transmission model and its parameters (supplied by the argument `params` as a list). In the example below we use a frequency biased transmission model where the magnitude and the direction of the bias is tuned by the parameter $b$ ($b<0$ for anticonformism, $b>0$ for conformism, and $b=0$ for an unbiased transmission model)

```{r}
generate_cultural_change(x=x,iniPop=ini,rr=rr,mu=0.01,params=list(b=0),model=frequencyBias)
```

### Creating a bespoke transmission model

The arguments `model` and `params` provides a fexible structure, enabling also the possibility to define custom models of cultural transission as an R function. This will need a minimum of two arguments; the relative or absolute frequency of cultural variants  `x` and the rate of innovation `mu`. The `generate_cultural_change()` will automatically supply these to values to the custom function along with any additional parameters defined in the argument `params`. The custom function should return the expected probability of selecting each of the supplied variants. 

The script below provides an example of custom function of a conformist transmission model where there is an additional probability $c$ of selecting the most common cultural variant.

```{r}
conformistBias <- function(x,mu,C)
{
      x = as.vector(x / sum(x)) # compute proportions
      k = length(x)
      i = which.max(x) #identify the most common variant
      transProb = (1-mu-C)*x # unbiased transmission component
      transProb[i] = transProb[i]+(C)/length(i) #conformist transmission component
      transProb[k] = transProb[k]+mu  #innovation component
      return(transProb)
}
```

which can be supplied as an argument for `generate_cultural_change()`:

```{r}
generate_cultural_change(x=x,iniPop=ini,rr=rr,mu=0.01,params=list(C=0.1),model=conformistBias)
```


# Approximate Bayesian Computation via the *EasyABC* package

The simulation model generated from the `genSim()` function enables direct comparison between observed adn expected variant frequencies under the assumption of a particular model. The figures below, for example, compares 500 estimates using $\mu=0.01$ and three different settings of $b$ ($b=0$,$b=-0.1$, and $b=0.1$).

```{r,fig.width=10,fig.height=4}
par(mfrow=c(1,3))
hist(replicate(500,sim(list(0.01,0))[1]),breaks=seq(0,1,0.05),border=NA,col='grey',xlab=paste0("Frequency of Variant ",colnames(x$cfreq)[1]),main='mu=0.01 and b=0')
abline(v=x$target.freq[1],lty=2)

hist(replicate(500,sim(list(0.01,0.1))[1]),breaks=seq(0,1,0.05),border=NA,col='grey',xlab=paste0("Frequency of Variant ",colnames(x$cfreq)[1]),main='mu=0.01 and b=0.1')
abline(v=x$target.freq[1],lty=2)

hist(replicate(500,sim(list(0.01,-0.1))[1]),breaks=seq(0,1,0.05),border=NA,col='grey',xlab=paste0("Frequency of Variant ",colnames(x$cfreq)[1]),main='mu=0.01 and b=-0.1')
abline(v=x$target.freq[1],lty=2)
```

The approximate bayesian computation framework expands this notion by measuring the euclidean distance between observed and _target_ cultural variants (i.e. the variants that were present during the first phase) for a random sample of parameter combinations drawn from user-defined _prior_ distributions. The parameters associated with the subset of simultions yielding the closest fit between observed and target frequencies becomes then the _posterior_ distribution indicating the most likely parameter combination. 

While it is possible to devise a routine to execute such inferential framework, we reccomend the use of the `EasyABC` package, which provides a variety of algorithms. The example below utilises the _rejection algorithm_ which is the most basic and simpest form of ABC where users just need to define the number of simulations (`nb_simul`) and the proportion of runs with the closest fit to thedata (`tol`). As for other ABC algorithm the function would need the simulation model (`model`), a list defining the parameter priors, and the target values (`summary_stat_target`). The example below examines our dataset `x` using our `sim` model with uniform prior for `mu` ranging between 0.001 and 0.01, and a uniform prior for `b`, ranging between -0.1 and 0.1:

```{r,results='hide'}
library(EasyABC)
res = ABC_rejection(model=sim,prior=list(prior_mu=c("unif",0.001,0.01),prior_b=c("unif",-0.1,0.1)),tol=0.01,nb_simul=100000,summary_stat_target=x$target.freq) 
```

## Analysing the ABC output

The output of the `ABC_rejection()` function contains a data.frame (`$param`) with the posterior samples. The function `plotPost()` provides a quick way to visualise both the marginal and the pairwise joint distribution of the posteriors:

```{r,fig.width=6,fig.height=6}
plotPost(res$param,pnames = c("mu","b"))
```

The `cTransmission` package provides also a routine for carrying out _posterior predictive checks_. This consists of making predictions using randomly selected parameters from the posterior distribution. The `predCheck()` funtion provides this routine and its output can be displayed to assess whether the model output can correctly match observed variant frequencies.   

```{r,fig.width=8,fig.height=5}
p=predCheck(x,sim,posterior=res$param)

par(mfrow=c(1,2))
plot(p,index=1)
plot(p,index=2)
```




